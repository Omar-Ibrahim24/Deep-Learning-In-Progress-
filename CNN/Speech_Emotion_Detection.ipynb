{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Mouting Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "aFdIAnLSPK3O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18200b05-f491-4db2-8be3-595dc2bca21b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "9m2fvfSIOoSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "import wave"
      ],
      "metadata": {
        "id": "rflqREmOPQvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEGWueipyqS0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Unzip Files\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/Crema.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/drive/MyDrive/Audio Files \")\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process Audios"
      ],
      "metadata": {
        "id": "mEnT01l5OrYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from types import AsyncGeneratorType\n",
        "import os\n",
        "\n",
        "path = \"/content/drive/MyDrive/Machine Learning/SER/Audio Files /\"\n",
        "\n",
        "sadness=[]\n",
        "angry=[]\n",
        "disgust=[]\n",
        "fear=[]\n",
        "happy=[]\n",
        "neutral=[]\n",
        "labels=[]\n",
        "for filename in os.listdir(path):\n",
        "    if \"SAD\" in filename:\n",
        "        sadness.append(filename)\n",
        "        labels.append(\"Sad\")\n",
        "\n",
        "    elif \"ANG\" in filename:\n",
        "        angry.append(filename)\n",
        "        labels.append(\"Angry\")\n",
        "    elif \"DIS\" in filename:\n",
        "        disgust.append(filename)\n",
        "        labels.append(\"Disgust\")\n",
        "    elif \"FEA\" in filename:\n",
        "        fear.append(filename)\n",
        "        labels.append(\"Fear\")\n",
        "    elif \"HAP\" in filename:\n",
        "        happy.append(filename)\n",
        "        labels.append(\"Happy\")\n",
        "\n",
        "    elif \"NEU\" in filename:\n",
        "        neutral.append(filename)\n",
        "        labels.append(\"Neutral\")"
      ],
      "metadata": {
        "id": "N9GHbfubTrp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"sadnes size\",len(sadness))\n",
        "print(\"angry size\",len(angry))\n",
        "print(\"disgust size\",len(disgust))\n",
        "print(\"fear size\",len(fear))\n",
        "print(\"happy size\",len(happy))\n",
        "print(\"neutral size\",len(neutral))\n",
        "print(\"labels size\",len(labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K95sDb1ZUnS0",
        "outputId": "f51ac40b-3eb4-450f-92c5-55abbcabb5ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sadnes size 1271\n",
            "angry size 1271\n",
            "disgust size 1271\n",
            "fear size 1271\n",
            "happy size 1271\n",
            "neutral size 1087\n",
            "labels size 7442\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Reading Audio Using Librosa Library\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load(path):\n",
        "  audios=[]\n",
        "  for filename in os.listdir(path): \n",
        "    y, sr = librosa.load(f'/content/drive/MyDrive/Machine Learning/SER/Audio Files /{filename}')\n",
        "    audios.append(y)\n",
        "  return audios,sr  \n",
        "\n",
        "\n",
        "  # plt.figure(figsize=(15,5))\n",
        "  # plt.plot(y);\n",
        "  # plt.title('Signal');\n",
        "  # plt.xlabel('Time (samples)');\n",
        "  # plt.ylabel('Amplitude');"
      ],
      "metadata": {
        "id": "wQj9nNlW-zHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Shift Pitch of Waveform\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def pitch_shifting(audios,sr):\n",
        "  pitch_shifted=[]\n",
        "  for i in range (len(audios)):\n",
        "      pitch_shifted.append(librosa.effects.pitch_shift(audios[i], sr=sr, n_steps=4))\n",
        "  return pitch_shifted\n",
        "\n",
        "\n",
        "#Time Stretching of Waveform\n",
        "def time_stretching(audios):\n",
        "  time_stretched=[]\n",
        "  for i in range (len(audios)):\n",
        "      time_stretched.append(librosa.effects.time_stretch(audios[i], rate=0.8))\n",
        "  return time_stretched\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nhotbn1Uswfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Zero Crossing Rate\n",
        "def zcr(data):\n",
        "    frame_length=2048\n",
        "    hop_length=512\n",
        "    zcr = librosa.feature.zero_crossing_rate(y=data, frame_length=frame_length, hop_length=hop_length)\n",
        "    return np.squeeze(zcr)\n",
        "\n",
        "\n",
        "def energy(data):\n",
        "    frame_length=2048\n",
        "    hop_length=512\n",
        "    en = np.array([np.sum(np.power(np.abs(data[hop:hop+frame_length]), 2)) for hop in range(0, data.shape[0], hop_length)])\n",
        "    return en / frame_length\n",
        "\n"
      ],
      "metadata": {
        "id": "160Galw9s1sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def concat(audio, time_stretched, pitch_shifted):\n",
        "#     # Concatenate the data\n",
        "#     concatenated_data = np.concatenate((audio, time_stretched, pitch_shifted), axis=0)\n",
        "#     return concatenated_data\n",
        "\n",
        "def padding(audios):\n",
        "  max_size=max(map(len,audios))\n",
        "  padded_audios = [np.pad(audio, (0, max_size - len(audio)), mode='constant') for audio in audios]\n",
        "\n",
        "  return padded_audios\n",
        "\n",
        "#       # sequence_list = [list(audio) for audio in audios]\n",
        "#       # print(type(sequence_list))\n",
        "#       # print(type(sequence_list[0]))\n",
        "#       # padded_data = pad_sequences(sequence_list, padding='post')\n",
        "\n"
      ],
      "metadata": {
        "id": "yoLkBsjvs3bE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audios,sr = load( \"/content/drive/MyDrive/Machine Learning/SER/Audio Files /\")"
      ],
      "metadata": {
        "id": "1TFQ1rVas89t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sound=[]\n",
        "for audio in audios:\n",
        "  data=np.array([])\n",
        "  data=np.hstack((data,zcr(audio),energy(audio)))\n",
        "  sound.append(data)\n"
      ],
      "metadata": {
        "id": "41LkDQvCMs3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sounds=padding(sound)"
      ],
      "metadata": {
        "id": "shwHzXExOOXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audios = np.vstack(padded_sounds)"
      ],
      "metadata": {
        "id": "TKU5W78bPBB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# audiosPad = padding(audios)\n",
        "# audios = np.vstack(audiosPad)"
      ],
      "metadata": {
        "id": "m_18i1o7utey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #@title Load Audios Concat Audios when session Crash\n",
        "# audios = np.load(\"/content/drive/MyDrive/Machine Learning/SER/Concat_Audios.npy\",allow_pickle=True)\n",
        "# #@title Tripling Output \n",
        "# labels=labels+labels+labels"
      ],
      "metadata": {
        "id": "_i_itFf_BJt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "# Convert string labels to numeric labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)"
      ],
      "metadata": {
        "id": "aUbHZnra23uL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(audios)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1X7tHPT1PaDf",
        "outputId": "ebd32a14-559b-4d8a-d242-164ea9bcdf7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7442"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(audios,labels, test_size=0.3, random_state=42, stratify=labels)\n",
        "x_train,x_val,y_train,y_val=train_test_split(x_train,y_train, test_size=0.05, random_state=42, stratify=y_train)\n",
        "# Check out the data\n",
        "print(f'X_train shape: {len(x_train)}')\n",
        "print(f'y_train shape: {len(y_train)}')\n",
        "print(f'X_test shape: {len(x_test)}')\n",
        "print(f'y_test shape: {len(y_test)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYau1AaxoipQ",
        "outputId": "0c5cd8ec-3086-4db3-b81e-f93239d9971e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: 4948\n",
            "y_train shape: 4948\n",
            "X_test shape: 2233\n",
            "y_test shape: 2233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title VGG\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense,Dropout\n",
        "\n",
        "# Create a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add 1D convolutional layers\n",
        "model.add(Conv1D(64, 3, activation='relu', padding='same', input_shape=(len(x_train[0]), 1)))\n",
        "model.add(Conv1D(64, 3, activation='relu', padding='same'))\n",
        "\n",
        "model.add(MaxPooling1D(2, strides=2))\n",
        "\n",
        "model.add(Conv1D(128, 3, activation='relu', padding='same'))\n",
        "model.add(Conv1D(128, 3, activation='relu', padding='same'))\n",
        "\n",
        "model.add(MaxPooling1D(2, strides=2))\n",
        "\n",
        "model.add(Conv1D(256, 3, activation='relu', padding='same'))\n",
        "model.add(Conv1D(256, 3, activation='relu', padding='same'))\n",
        "model.add(Conv1D(256, 3, activation='relu', padding='same'))\n",
        "\n",
        "model.add(MaxPooling1D(2, strides=2))\n",
        "\n",
        "model.add(Conv1D(512, 3, activation='relu', padding='same'))\n",
        "model.add(Conv1D(512, 3, activation='relu', padding='same'))\n",
        "model.add(Conv1D(512, 3, activation='relu', padding='same'))\n",
        "\n",
        "model.add(MaxPooling1D(2, strides=2))\n",
        "\n",
        "model.add(Conv1D(512, 3, activation='relu', padding='same'))\n",
        "model.add(Conv1D(512, 3, activation='relu', padding='same'))\n",
        "model.add(Conv1D(512, 3, activation='relu', padding='same'))\n",
        "\n",
        "model.add(MaxPooling1D(2, strides=2))\n",
        "\n",
        "# Flatten the feature maps\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(units=256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "1WGifKvvq0yA",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Simple Architecture\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Convolutional layers\n",
        "model.add(Conv1D(64, 3, activation='relu', padding='same', input_shape=(len(x_train[0]), 1)))\n",
        "\n",
        "model.add(MaxPooling1D(2, strides=2))\n",
        "\n",
        "model.add(Conv1D(128, 3, activation='relu', padding='same'))\n",
        "\n",
        "model.add(MaxPooling1D(2, strides=2))\n",
        "\n",
        "model.add(Conv1D(256, kernel_size=3, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Conv1D(512, kernel_size=3, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "# Flatten the output for the fully connected layers\n",
        "model.add(Flatten())\n",
        "\n",
        "# Fully connected layers\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(6, activation='softmax'))  # num_classes is the number of output classes\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qa4oEr-D4Y8i",
        "outputId": "075327bb-2ebb-4503-feb4-e47fe9eabbc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_8 (Conv1D)           (None, 432, 64)           256       \n",
            "                                                                 \n",
            " max_pooling1d_8 (MaxPooling  (None, 216, 64)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, 216, 128)          24704     \n",
            "                                                                 \n",
            " max_pooling1d_9 (MaxPooling  (None, 108, 128)         0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_10 (Conv1D)          (None, 106, 256)          98560     \n",
            "                                                                 \n",
            " max_pooling1d_10 (MaxPoolin  (None, 53, 256)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_11 (Conv1D)          (None, 51, 512)           393728    \n",
            "                                                                 \n",
            " max_pooling1d_11 (MaxPoolin  (None, 25, 512)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 256)               3277056   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,827,974\n",
            "Trainable params: 3,827,974\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import categorical_crossentropy\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "-xPquxSAr3jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Train the model \n",
        "\n",
        "model.fit(x_train, y_train, batch_size=16, epochs=50, validation_data=(x_val, y_val))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBovq6iIxtDp",
        "outputId": "225e9495-b86f-4a9c-8e6b-75e55f3483b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "310/310 [==============================] - 2s 5ms/step - loss: 1.7913 - accuracy: 0.1645 - val_loss: 1.7649 - val_accuracy: 0.2490\n",
            "Epoch 2/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.7301 - accuracy: 0.2603 - val_loss: 1.6698 - val_accuracy: 0.2797\n",
            "Epoch 3/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.6429 - accuracy: 0.3153 - val_loss: 1.6108 - val_accuracy: 0.3487\n",
            "Epoch 4/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.6008 - accuracy: 0.3323 - val_loss: 1.6327 - val_accuracy: 0.3333\n",
            "Epoch 5/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.5825 - accuracy: 0.3438 - val_loss: 1.5603 - val_accuracy: 0.3333\n",
            "Epoch 6/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.5791 - accuracy: 0.3446 - val_loss: 1.5469 - val_accuracy: 0.3448\n",
            "Epoch 7/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.5698 - accuracy: 0.3543 - val_loss: 1.5568 - val_accuracy: 0.3678\n",
            "Epoch 8/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.5520 - accuracy: 0.3614 - val_loss: 1.5693 - val_accuracy: 0.3640\n",
            "Epoch 9/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.5447 - accuracy: 0.3654 - val_loss: 1.6064 - val_accuracy: 0.3448\n",
            "Epoch 10/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.5285 - accuracy: 0.3686 - val_loss: 1.5546 - val_accuracy: 0.3487\n",
            "Epoch 11/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.5221 - accuracy: 0.3650 - val_loss: 1.5481 - val_accuracy: 0.3602\n",
            "Epoch 12/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.5098 - accuracy: 0.3808 - val_loss: 1.5500 - val_accuracy: 0.3563\n",
            "Epoch 13/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.4949 - accuracy: 0.3850 - val_loss: 1.5443 - val_accuracy: 0.3563\n",
            "Epoch 14/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.4942 - accuracy: 0.3949 - val_loss: 1.5787 - val_accuracy: 0.3678\n",
            "Epoch 15/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.4849 - accuracy: 0.4000 - val_loss: 1.5440 - val_accuracy: 0.3640\n",
            "Epoch 16/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.4680 - accuracy: 0.4032 - val_loss: 1.5274 - val_accuracy: 0.3563\n",
            "Epoch 17/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.4470 - accuracy: 0.4078 - val_loss: 1.5700 - val_accuracy: 0.3640\n",
            "Epoch 18/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.4542 - accuracy: 0.4076 - val_loss: 1.5111 - val_accuracy: 0.3831\n",
            "Epoch 19/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.4341 - accuracy: 0.4181 - val_loss: 1.5271 - val_accuracy: 0.4100\n",
            "Epoch 20/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.4169 - accuracy: 0.4285 - val_loss: 1.5505 - val_accuracy: 0.3525\n",
            "Epoch 21/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.4086 - accuracy: 0.4313 - val_loss: 1.5556 - val_accuracy: 0.3755\n",
            "Epoch 22/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.3900 - accuracy: 0.4384 - val_loss: 1.5402 - val_accuracy: 0.3716\n",
            "Epoch 23/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.3761 - accuracy: 0.4331 - val_loss: 1.5431 - val_accuracy: 0.3678\n",
            "Epoch 24/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.3613 - accuracy: 0.4511 - val_loss: 1.5233 - val_accuracy: 0.3908\n",
            "Epoch 25/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.3496 - accuracy: 0.4529 - val_loss: 1.5939 - val_accuracy: 0.3563\n",
            "Epoch 26/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.3125 - accuracy: 0.4644 - val_loss: 1.6159 - val_accuracy: 0.3640\n",
            "Epoch 27/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.3061 - accuracy: 0.4691 - val_loss: 1.6524 - val_accuracy: 0.3525\n",
            "Epoch 28/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.2892 - accuracy: 0.4814 - val_loss: 1.5723 - val_accuracy: 0.3640\n",
            "Epoch 29/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.2655 - accuracy: 0.4915 - val_loss: 1.6153 - val_accuracy: 0.3716\n",
            "Epoch 30/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.2521 - accuracy: 0.5004 - val_loss: 1.6038 - val_accuracy: 0.3793\n",
            "Epoch 31/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.2389 - accuracy: 0.4998 - val_loss: 1.6448 - val_accuracy: 0.3908\n",
            "Epoch 32/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.2118 - accuracy: 0.5129 - val_loss: 1.6830 - val_accuracy: 0.3487\n",
            "Epoch 33/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.2015 - accuracy: 0.5146 - val_loss: 1.6592 - val_accuracy: 0.3333\n",
            "Epoch 34/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.1754 - accuracy: 0.5285 - val_loss: 1.6859 - val_accuracy: 0.3410\n",
            "Epoch 35/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.1480 - accuracy: 0.5457 - val_loss: 1.7236 - val_accuracy: 0.3525\n",
            "Epoch 36/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.1507 - accuracy: 0.5406 - val_loss: 1.7502 - val_accuracy: 0.3602\n",
            "Epoch 37/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.1039 - accuracy: 0.5629 - val_loss: 1.7705 - val_accuracy: 0.3678\n",
            "Epoch 38/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.0914 - accuracy: 0.5637 - val_loss: 1.8903 - val_accuracy: 0.3333\n",
            "Epoch 39/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.0937 - accuracy: 0.5622 - val_loss: 1.7692 - val_accuracy: 0.3448\n",
            "Epoch 40/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.0593 - accuracy: 0.5831 - val_loss: 1.9200 - val_accuracy: 0.3295\n",
            "Epoch 41/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.0481 - accuracy: 0.5863 - val_loss: 1.8945 - val_accuracy: 0.3372\n",
            "Epoch 42/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.0267 - accuracy: 0.5934 - val_loss: 1.9696 - val_accuracy: 0.3640\n",
            "Epoch 43/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 1.0173 - accuracy: 0.5972 - val_loss: 1.9725 - val_accuracy: 0.3487\n",
            "Epoch 44/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 0.9961 - accuracy: 0.5978 - val_loss: 1.9133 - val_accuracy: 0.3295\n",
            "Epoch 45/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 0.9694 - accuracy: 0.6128 - val_loss: 2.0919 - val_accuracy: 0.3333\n",
            "Epoch 46/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 0.9776 - accuracy: 0.6130 - val_loss: 1.9737 - val_accuracy: 0.3372\n",
            "Epoch 47/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 0.9431 - accuracy: 0.6219 - val_loss: 2.0714 - val_accuracy: 0.3218\n",
            "Epoch 48/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 0.9249 - accuracy: 0.6326 - val_loss: 2.0834 - val_accuracy: 0.3180\n",
            "Epoch 49/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 0.8989 - accuracy: 0.6417 - val_loss: 2.1581 - val_accuracy: 0.3448\n",
            "Epoch 50/50\n",
            "310/310 [==============================] - 1s 4ms/step - loss: 0.8888 - accuracy: 0.6469 - val_loss: 2.2851 - val_accuracy: 0.3180\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4b682d5720>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vHzxCwLx8LMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "# model.fit_ge(X_train,y_train,epochs=10 ,batch_size=128)\n",
        "# eval the model\n",
        "loss,accuracy = model.evaluate(x_test,y_test)\n",
        "\n",
        "# Make predictions with the trained model\n",
        "predictions = model.predict(x_test)"
      ],
      "metadata": {
        "id": "lMChQn-gtMcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title dont do this at home\n",
        "# Import necessary libraries\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio\n",
        "from ipywidgets import interactive\n",
        "import soundfile as sf\n",
        "import librosa.display\n",
        "from IPython.display import Audio\n",
        "# sf.write('input.wav', y, 44100)\n",
        "# Load the input recording\n",
        "data, sr = librosa.load('/content/drive/MyDrive/Audio Files /1001_DFA_HAP_XX.wav', sr=44100)\n",
        "\n",
        "# Display the input audio file in time domain\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(data)\n",
        "plt.title('Input Audio Waveform')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.show()\n",
        "\n",
        "# Set frame length, hop length, and ZCR threshold\n",
        "framelength = 2000\n",
        "hoplength = int(framelength/4)\n",
        "zcr_threshold = 0.1\n",
        "\n",
        "# Compute ZCR of the audio signal using librosa\n",
        "zcr = librosa.feature.zero_crossing_rate(data, frame_length=framelength, hop_length=hoplength)[0]\n",
        "\n",
        "# Plot the computed ZCR\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(zcr)\n",
        "plt.axhline(y=zcr_threshold, color='r', linestyle='-')\n",
        "plt.title('Zero-Crossing Rate')\n",
        "plt.xlabel('Frame Index')\n",
        "plt.ylabel('ZCR')\n",
        "plt.show()\n",
        "\n",
        "# Create 'data' array containing the input file\n",
        "data = np.array(data)\n",
        "\n",
        "# Set frame start and end positions to 0\n",
        "framestart = 0\n",
        "frameend = 0\n",
        "count = 0\n",
        "count2=0\n",
        "# Iterate through 'zcr' array with 'index' as the iterator\n",
        "for index in range(len(zcr)):\n",
        "    # If 'zcr' value at 'index' is less than the threshold, increment frame start and end positions by hop length\n",
        "    \n",
        "    if zcr[index] < zcr_threshold:\n",
        "        count+=1\n",
        "        framestart += hoplength\n",
        "        frameend += hoplength\n",
        "        \n",
        "    # If frame end position exceeds length of 'data' array, break loop\n",
        "    elif frameend + hoplength > len(data):\n",
        "        break\n",
        "    # Otherwise, set values in 'data' array to 2 within the current frame\n",
        "    else:\n",
        "        count2+=1\n",
        "        for i in range(framestart, frameend - hoplength):\n",
        "            print(\"LOLO\")\n",
        "            data[i] = 2\n",
        "        # Move on to next frame\n",
        "        framestart += hoplength\n",
        "        frameend += hoplength\n",
        "    \n",
        "# Iterate through 'data' array and list the values that are not 2 in 'output' array\n",
        "print(count)\n",
        "print(count2)\n",
        "output = []\n",
        "\n",
        "for i in range(len(data)):\n",
        "    if data[i] != 2:\n",
        "        output.append(data[i])\n",
        "output = np.array(output)\n",
        "\n",
        "# Plot the 'output' array, representing the output waveform with the noise removed\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(output)\n",
        "plt.title('Output Audio Waveform')\n",
        "plt.xlabel('Time (samples)')\n",
        "# Save the output audio file\n",
        "\n",
        "sf.write('output.wav', output, sr)\n",
        "\n",
        "\n",
        "print(len(data))\n",
        "print(len(output))\n",
        "\n",
        "# Play both input and output audio files using widgets\n",
        "input_widget = Audio(data, rate=sr)\n",
        "output_widget = Audio(output, rate=sr)\n",
        "\n",
        "def play_audio(input_audio, output_audio):\n",
        "    display(input_audio)\n",
        "    display(output_audio)\n",
        "\n",
        "play_audio(input_widget,output_widget)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "bwxqJDjj_pGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title dont do this at home\n",
        "# compute STAZCR\n",
        "\n",
        "stazcr = librosa.feature.zero_crossing_rate(y)\n",
        "\n",
        "# set thresholds\n",
        "voiced_threshold = 0.05\n",
        "unvoiced_threshold = 0.2\n",
        "\n",
        "# segment signal based on thresholds\n",
        "segments = []\n",
        "start = 0\n",
        "for i in range(len(stazcr[0])):\n",
        "    if stazcr[0][i] < voiced_threshold:\n",
        "        if start == 0:\n",
        "            start = i\n",
        "    elif stazcr[0][i] > unvoiced_threshold:\n",
        "        if start != 0:\n",
        "            segments.append((start, i))\n",
        "            start = 0\n",
        "if start != 0:\n",
        "    segments.append((start, len(stazcr[0])))\n",
        "\n",
        "# extract voiced and unvoiced segments\n",
        "voiced_segments = []\n",
        "unvoiced_segments = []\n",
        "for segment in segments:\n",
        "    if stazcr[0][segment[0]] < voiced_threshold:\n",
        "        # voiced segment\n",
        "        voiced_segments.append(y[segment[0]:segment[1]])\n",
        "    else:\n",
        "        # unvoiced segment\n",
        "        unvoiced_segments.append(y[segment[0]:segment[1]])\n",
        "\n",
        "# print number of voiced and unvoiced segments\n",
        "print('Number of voiced segments:', len(voiced_segments))\n",
        "print('Number of unvoiced segments:', len(unvoiced_segments))"
      ],
      "metadata": {
        "id": "ctdwVdB3baal",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load audio file\n",
        "# y, sr = librosa.load('/content/1001_DFA_ANG_XX.wav')\n",
        "\n",
        "# Compute mel spectrogram\n",
        "n_fft = 2048\n",
        "hop_length = 512\n",
        "n_mels = 128\n",
        "fmin = 20\n",
        "fmax = sr // 2\n",
        "S = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, fmin=fmin, fmax=fmax)\n",
        "\n",
        "# Convert to decibels\n",
        "S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "# Plot spectrogram\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(S_dB, x_axis='time', y_axis='mel', sr=sr, fmin=fmin, fmax=fmax)\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.title('Mel-frequency spectrogram')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FAq4eJED-SXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hear Sound\n",
        "from IPython.display import Audio\n",
        "wn = Audio('/content/drive/MyDrive/Audio Files /1007_IWW_ANG_XX.wav', autoplay=True)\n",
        "display(wn)"
      ],
      "metadata": {
        "id": "M5ffLTHNzhF0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}